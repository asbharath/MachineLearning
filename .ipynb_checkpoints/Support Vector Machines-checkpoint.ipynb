{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Support Vector Machines\n",
    "\n",
    "Hyperplane - In a $p$ dimensional space, a hyperplane is a flat affine subspace of dimension $p-1$.\n",
    "\n",
    "equation of hyperplane $ \\beta_0 + \\beta_1X_1 + \\beta_2X_2 = 0 $\n",
    "\n",
    "$X_1,X_2$ are vectors of in 2 dimensional space. \n",
    "\n",
    "##### Maximum Marginal Classifier \n",
    ">is separating hyperplane for which the margin is largest - that is, the hyperplane that has the farthest minimum distance to the training observations. \n",
    "\n",
    "requires classes to be separated by a linear boundary, also called optimal separating hyperplane.\n",
    "\n",
    "    1. Maximize the Margin between the support vectors\n",
    "    \n",
    "$\\text{maximize M } \\beta_0,\\beta_1,.....,\\beta_p \n",
    "\\text{ subject to }\\sum^{p}_{j=1}\\beta_j^2 = 1$\n",
    "\n",
    "The gap in the maximum margin hyperplane is seen as a measure of our confidence that the observation was correctly classified. \n",
    "\n",
    "**Negatives**\n",
    "In the case where the observations are not separable and a new test observation added may reduce the maximal margin which may not generalize well.This approach could be very sensitive and may overfit the training data. \n",
    "\n",
    "##### Support Vector Classifer\n",
    "also called soft-margin classifier\n",
    ">an extension of the above which fits in broad range of cases. \n",
    "    \n",
    "    Greater robustness to individual observation\n",
    "    Better classification of most of the training observations.\n",
    "    \n",
    "Rather than seeking the largest possible margin to make sure every observations is not only in the correct side of the hyperplan but also on the correct side of the margin, we allow few observatios to be incorrectly classified or even be on the wrong side of the hyperplane. \n",
    "\n",
    "$$ \\text{maximize M }\\beta_0,\\beta_1,....,\\beta_p $$\n",
    "$$ \\text{subject to } \\sum_{j=1}^{p} \\beta_j^2 = 1$$\n",
    "$$ y_i(\\beta_0+\\beta_1x_i1+\\beta_2x_i2+....+\\beta_px_ip) \\ge M(1-\\epsilon_i)$$\n",
    "$$ \\epsilon_i \\ge 0, \\sum_{i=1}^n\\epsilon_i \\le C $$\n",
    "\n",
    "$\\epsilon$ is a slack variable that allow individual observations to be on the wrong side of the hyperplane or margin. \n",
    "when $\\epsilon$ = 0 then observations is located on the correct side of the hyperplan or margin. <br>\n",
    "when $\\epsilon$ > 0 then the ith observation is located on the wrong side of the margin. <br>\n",
    "when $\\epsilon$ > 1 then the ith observation is located on the wrong side of the hyperplane<br>\n",
    "\n",
    "$C$ bounds the sum of the $\\epsilon_i$ and determines the severity of the violation. \n",
    "\n",
    "If $C$ = 0 then no budget for violations and the equation becomes equal to a maximum marginal classifier. \n",
    "\n",
    "Observations that lie on the margin or the wrong side of margin are called support vectors. \n",
    "\n",
    "**negatives** \n",
    "works well with 2 classes and boundary between the 2 classes are linear. if non-linear this approach does not work. \n",
    "\n",
    "##### Support Vector Machines\n",
    "\n",
    ">extension of support vector classifer to accommodate non-linear classes boundaries. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
