{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "##### What is NLP (Natual Language Processing) \n",
    "\n",
    "Is a subfield of computer science, information engineering and AI concerned with interaction between computers and human(natural) languages, in particular how to program computers to process and analyze large amounts of natural language data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### NLP uses\n",
    "Speech Recognition \n",
    "Document summarization\n",
    "Sentiment Analysis\n",
    "ChatBots\n",
    "Predicting the genre of the book \n",
    "Question Answering \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentence tokenization\n",
    ">The process of separating sentences within paragraphs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'Natural language processing (NLP) is a subfield of computer science, information engineering, and artificial intelligence concerned with the interactions between computers and human (natural) languages, in particular how to program computers to process and analyze large amounts of natural language data. Challenges in natural language processing frequently involve speech recognition, natural language understanding, and natural language generation.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Natural language processing (NLP) is a subfield of computer science, information engineering, and artificial intelligence concerned with the interactions between computers and human (natural) languages, in particular how to program computers to process and analyze large amounts of natural language data.\n",
      "\n",
      "Challenges in natural language processing frequently involve speech recognition, natural language understanding, and natural language generation.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from nltk import sent_tokenize\n",
    "sent_tok = sent_tokenize(text)\n",
    "for sentence in sent_tok:\n",
    "    print(sentence)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word tokenization\n",
    ">The process of separating words within the sentences. <br>\n",
    "Sometimes single a word is split with a space "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Natural',\n",
       " 'language',\n",
       " 'processing',\n",
       " '(',\n",
       " 'NLP',\n",
       " ')',\n",
       " 'is',\n",
       " 'a',\n",
       " 'subfield',\n",
       " 'of',\n",
       " 'computer',\n",
       " 'science',\n",
       " ',',\n",
       " 'information',\n",
       " 'engineering',\n",
       " ',',\n",
       " 'and',\n",
       " 'artificial',\n",
       " 'intelligence',\n",
       " 'concerned',\n",
       " 'with',\n",
       " 'the',\n",
       " 'interactions',\n",
       " 'between',\n",
       " 'computers',\n",
       " 'and',\n",
       " 'human',\n",
       " '(',\n",
       " 'natural',\n",
       " ')',\n",
       " 'languages',\n",
       " ',',\n",
       " 'in',\n",
       " 'particular',\n",
       " 'how',\n",
       " 'to',\n",
       " 'program',\n",
       " 'computers',\n",
       " 'to',\n",
       " 'process',\n",
       " 'and',\n",
       " 'analyze',\n",
       " 'large',\n",
       " 'amounts',\n",
       " 'of',\n",
       " 'natural',\n",
       " 'language',\n",
       " 'data',\n",
       " '.',\n",
       " 'Challenges',\n",
       " 'in',\n",
       " 'natural',\n",
       " 'language',\n",
       " 'processing',\n",
       " 'frequently',\n",
       " 'involve',\n",
       " 'speech',\n",
       " 'recognition',\n",
       " ',',\n",
       " 'natural',\n",
       " 'language',\n",
       " 'understanding',\n",
       " ',',\n",
       " 'and',\n",
       " 'natural',\n",
       " 'language',\n",
       " 'generation',\n",
       " '.']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "word_tok = word_tokenize(text)\n",
    "[word for word in word_tok]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Text Lemmatization and Stemming\n",
    "> documents may contain different form of a word (running, run, runs) and can have related words with similar meaning (nation, national, nationality) <br>\n",
    "Reduce different form of words and words to its base form.\n",
    "\n",
    ">Stemming often includes removal of the derivational affixes. (if stemming is applied to the word 'see', it might return just 's'). stemmer operates without knowledge of the context. \n",
    ">Lemmatization aims to remove the inflectional endings only to return the base or dictionary form of a word known as **lemma**. (if lemmatization is applied to the word 'see', it would attempt to return either see or saw depending on the use of the token. \n",
    "\n",
    "Def: [Inflection](https://en.wikipedia.org/wiki/Inflection): Process of word formation in which a word is modified to express grammatical categories such as tense, case, voice, aspect, person, number, gender and mood. Also referred as conjugation. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemmer see\n",
      "Lemmatizer see\n",
      "\n",
      "Stemmer spun\n",
      "Lemmatizer spin\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer,WordNetLemmatizer\n",
    "# import nltk\n",
    "# nltk.download('wordnet')\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "\n",
    "def comp_stem_lemma(stemmer, lemmatizer, word, pos):\n",
    "    print(\"Stemmer\",stemmer.stem(word))\n",
    "    print(\"Lemmatizer\",lemmatizer.lemmatize(word,pos))\n",
    "    print()\n",
    "    \n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "comp_stem_lemma(stemmer,lemmatizer,word=\"seeing\",pos=wordnet.VERB)\n",
    "comp_stem_lemma(stemmer,lemmatizer,word=\"spun\",pos=wordnet.VERB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stop Words\n",
    "Words such as 'and' , 'the' , 'a' in a language. These add to noise when ML is applied. \n",
    "NLTK has a predefined list of stopwords "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "# nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Natural', 'language', 'processing', '(', 'NLP', ')', 'subfield', 'computer', 'science', ',', 'information', 'engineering', ',', 'artificial', 'intelligence', 'concerned', 'interactions', 'computers', 'human', '(', 'natural', ')', 'languages', ',', 'particular', 'program', 'computers', 'process', 'analyze', 'large', 'amounts', 'natural', 'language', 'data', '.', 'Challenges', 'natural', 'language', 'processing', 'frequently', 'involve', 'speech', 'recognition', ',', 'natural', 'language', 'understanding', ',', 'natural', 'language', 'generation', '.']\n"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "words = nltk.word_tokenize(text)\n",
    "without_stop_words = [word for word in words if word not in stop_words]\n",
    "print(without_stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### [Regular Expression](https://digitalfortress.tech/tricks/top-15-commonly-used-regex/)<br>\n",
    "[Python document for RegEx](https://docs.python.org/3/library/re.html?highlight=regex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Natural language processing  NLP  is a subfield of computer science  information engineering  and artificial intelligence concerned with the interactions between computers and human  natural  languages  in particular how to program computers to process and analyze large amounts of natural language data  Challenges in natural language processing frequently involve speech recognition  natural language understanding  and natural language generation \n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "pattern = r\"[^\\w]\"\n",
    "print(re.sub(pattern,\" \",text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Playing with regex <br> \n",
    "[link1](https://regexr.com/)<br>\n",
    "[link2](https://regex101.com/)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bag-of-words \n",
    "Bag of words is a simplying representation used in natural language processing and information retrieval(IR). In this model, a text is represented as the bag(multiset) of its words, disregarding grammer and even word order but keeping multiplicity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 1:  open the document \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Natural language processing (NLP) is a subfield of computer science, information engineering, and artificial intelligence concerned with the interactions between computers and human (natural) languages, in particular how to program computers to process and analyze large amounts of natural language data. ', '', 'Challenges in natural language processing frequently involve speech recognition, natural language understanding, and natural language generation.']\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "path = os.path.join(os.getcwd() + \"\\\\nlp.txt\")\n",
    "with open(path,'r') as f:\n",
    "    document = f.read().splitlines()\n",
    "print(document)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 2:  Desiginig the vocabulary\n",
    "using sklearn's count vectorizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vectorizer = CountVectorizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 3:  Creating document vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_of_words = count_vectorizer.fit_transform(document)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 4: converting the bag-of-words model as a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>amounts</th>\n",
       "      <th>analyze</th>\n",
       "      <th>and</th>\n",
       "      <th>artificial</th>\n",
       "      <th>between</th>\n",
       "      <th>challenges</th>\n",
       "      <th>computer</th>\n",
       "      <th>computers</th>\n",
       "      <th>concerned</th>\n",
       "      <th>data</th>\n",
       "      <th>...</th>\n",
       "      <th>processing</th>\n",
       "      <th>program</th>\n",
       "      <th>recognition</th>\n",
       "      <th>science</th>\n",
       "      <th>speech</th>\n",
       "      <th>subfield</th>\n",
       "      <th>the</th>\n",
       "      <th>to</th>\n",
       "      <th>understanding</th>\n",
       "      <th>with</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 39 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   amounts  analyze  and  artificial  between  challenges  computer  \\\n",
       "0        1        1    3           1        1           0         1   \n",
       "1        0        0    0           0        0           0         0   \n",
       "2        0        0    1           0        0           1         0   \n",
       "\n",
       "   computers  concerned  data  ...  processing  program  recognition  science  \\\n",
       "0          2          1     1  ...           1        1            0        1   \n",
       "1          0          0     0  ...           0        0            0        0   \n",
       "2          0          0     0  ...           1        0            1        0   \n",
       "\n",
       "   speech  subfield  the  to  understanding  with  \n",
       "0       0         1    1   2              0     1  \n",
       "1       0         0    0   0              0     0  \n",
       "2       1         0    0   0              1     0  \n",
       "\n",
       "[3 rows x 39 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "feature_names = count_vectorizer.get_feature_names()\n",
    "pd.DataFrame(bag_of_words.toarray(),columns=feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complexity of the bag-of-model comes in deciding how to design the vocabulary of known words(tokens) and how to score presence of known words\n",
    "\n",
    "its better to decrese the known words using the text cleaning techniques\n",
    "\n",
    "    1.Ignoring the case of the words\n",
    "    2.Ignorning punctuation\n",
    "    3.removing the stop words from our documents\n",
    "    4.Reducing words to their base form (text lemmatization and stemming) \n",
    "    5.Fixing misspelled words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### n-grams \n",
    "is a contiguous sequence of n items from a given sample of text or speech <br>\n",
    "can be phonemes, syllables, letters, words or base pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Scoring words\n",
    "once we have our vocabulary of known words, we need to score the occurance of the words in our data. \n",
    "\n",
    "    Counts \n",
    "    Frequencies \n",
    "    \n",
    "Downside with scoring word frequency is that most frequent words in the document will have highest scores, which may not contain informational gain to the model compared with some rarer and domain-specific words.\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### TF-IDF  (Term frequency - Inverse document frequency)\n",
    "is statistical measure used to evaluate the importance of a word to a document in a collection (corpus) \n",
    "\n",
    "To fix the above, penalize the frequent words across all the documents. This approach is called TF-IDF.\n",
    "\n",
    "\\begin{align}\n",
    "W_{x,y} = tf_{x,y} \\times log(\\frac{N}{df_x}) \\\\\n",
    "\\\\\n",
    "tf_{x,y} = \\text{frequency of x in y} \\\\\n",
    "df_x = \\text{number of documents containing x} \\\\\n",
    "N = \\text{total number of documents}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>amounts</th>\n",
       "      <th>analyze</th>\n",
       "      <th>and</th>\n",
       "      <th>artificial</th>\n",
       "      <th>between</th>\n",
       "      <th>challenges</th>\n",
       "      <th>computer</th>\n",
       "      <th>computers</th>\n",
       "      <th>concerned</th>\n",
       "      <th>data</th>\n",
       "      <th>...</th>\n",
       "      <th>processing</th>\n",
       "      <th>program</th>\n",
       "      <th>recognition</th>\n",
       "      <th>science</th>\n",
       "      <th>speech</th>\n",
       "      <th>subfield</th>\n",
       "      <th>the</th>\n",
       "      <th>to</th>\n",
       "      <th>understanding</th>\n",
       "      <th>with</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.141589</td>\n",
       "      <td>0.141589</td>\n",
       "      <td>0.323047</td>\n",
       "      <td>0.141589</td>\n",
       "      <td>0.141589</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.141589</td>\n",
       "      <td>0.283178</td>\n",
       "      <td>0.141589</td>\n",
       "      <td>0.141589</td>\n",
       "      <td>...</td>\n",
       "      <td>0.107682</td>\n",
       "      <td>0.141589</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.141589</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.141589</td>\n",
       "      <td>0.141589</td>\n",
       "      <td>0.283178</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.141589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.173808</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.228537</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.173808</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.228537</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.228537</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.228537</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 39 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    amounts   analyze       and  artificial   between  challenges  computer  \\\n",
       "0  0.141589  0.141589  0.323047    0.141589  0.141589    0.000000  0.141589   \n",
       "1  0.000000  0.000000  0.000000    0.000000  0.000000    0.000000  0.000000   \n",
       "2  0.000000  0.000000  0.173808    0.000000  0.000000    0.228537  0.000000   \n",
       "\n",
       "   computers  concerned      data  ...  processing   program  recognition  \\\n",
       "0   0.283178   0.141589  0.141589  ...    0.107682  0.141589     0.000000   \n",
       "1   0.000000   0.000000  0.000000  ...    0.000000  0.000000     0.000000   \n",
       "2   0.000000   0.000000  0.000000  ...    0.173808  0.000000     0.228537   \n",
       "\n",
       "    science    speech  subfield       the        to  understanding      with  \n",
       "0  0.141589  0.000000  0.141589  0.141589  0.283178       0.000000  0.141589  \n",
       "1  0.000000  0.000000  0.000000  0.000000  0.000000       0.000000  0.000000  \n",
       "2  0.000000  0.228537  0.000000  0.000000  0.000000       0.228537  0.000000  \n",
       "\n",
       "[3 rows x 39 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "values = tfidf_vectorizer.fit_transform(document)\n",
    "\n",
    "feature_names = tfidf_vectorizer.get_feature_names()\n",
    "pd.DataFrame(values.toarray(),columns=feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Crash Blossems - A sentence can be interpreted into 2 different meanings"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
