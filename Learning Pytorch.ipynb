{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('dark_background')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    '''\n",
    "    Sigmoid activation function \n",
    "    return x object tensor.Tensor\n",
    "    '''\n",
    "    return 1/(1+torch.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(7) # random seed\n",
    "\n",
    "features = torch.randn(1,3)\n",
    "weights = torch.randn_like(features)\n",
    "bias = torch.randn(1,1)\n",
    "\n",
    "n_input = features.shape[1]\n",
    "n_hidden = 2\n",
    "n_output = 1\n",
    "\n",
    "w1 = torch.randn(n_input,n_hidden)\n",
    "w2 = torch.randn(n_hidden,n_output)\n",
    "\n",
    "b1 = torch.randn(1,n_hidden)\n",
    "b2 = torch.randn(1,n_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ y = \\sigma(W^TX + b) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing out reshaping the matrix "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Better to not use **reshape** as sometimes it copies the data into another part of the memory.\n",
    "**resize_** performs an inplace operation on the same variable and if the new shape is less the older data is not removed from the memory location, if the new shape is more than the older data some parts would have uninitialized values. \n",
    "view is a better option as it would return the data reshaped and will take the same memory location to save space. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5720]])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigmoid(torch.mm(features,weights.view(3,1)) + bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.8371]])\n"
     ]
    }
   ],
   "source": [
    "h = sigmoid(torch.mm(features,w1) + b1)\n",
    "output = sigmoid(torch.mm(h,w2) + b2)\n",
    "print(output) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "a = np.random.randn(4,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = torch.from_numpy(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0156,  0.7535, -1.1550, -0.5861, -0.4384],\n",
       "        [ 0.6311,  0.6647, -0.2380, -0.1670, -0.3565],\n",
       "        [ 0.6042,  0.8750,  1.1624,  1.6485, -0.3822],\n",
       "        [-2.2467, -0.8893,  1.0070, -0.9340, -0.1267]], dtype=torch.float64)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.01562847,  0.75348791, -1.15501931, -0.5861146 , -0.43843942],\n",
       "       [ 0.6311309 ,  0.66471633, -0.23800402, -0.16700245, -0.35652653],\n",
       "       [ 0.60419477,  0.87499385,  1.16241269,  1.64852504, -0.38219743],\n",
       "       [-2.24673224, -0.88926795,  1.00697215, -0.93398522, -0.12669909]])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0313,  1.5070, -2.3100, -1.1722, -0.8769],\n",
       "        [ 1.2623,  1.3294, -0.4760, -0.3340, -0.7131],\n",
       "        [ 1.2084,  1.7500,  2.3248,  3.2971, -0.7644],\n",
       "        [-4.4935, -1.7785,  2.0139, -1.8680, -0.2534]], dtype=torch.float64)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.mul_(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.03125695,  1.50697581, -2.31003862, -1.17222921, -0.87687885],\n",
       "       [ 1.26226179,  1.32943266, -0.47600803, -0.33400489, -0.71305305],\n",
       "       [ 1.20838955,  1.74998769,  2.32482537,  3.29705007, -0.76439486],\n",
       "       [-4.49346449, -1.7785359 ,  2.01394429, -1.86797044, -0.25339819]])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the MNIST via pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets,transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting the batch size\n",
    "batch_size = 32\n",
    "\n",
    "trainset = datasets.MNIST('MNIST/', \n",
    "                               train=True, \n",
    "                               download=True, \n",
    "                               transform=transforms.ToTensor())\n",
    "\n",
    "validationset = datasets.MNIST('MNIST/', \n",
    "                                    train=False, \n",
    "                                    transform=transforms.ToTensor())\n",
    "\n",
    "trainset_loader = torch.utils.data.DataLoader(dataset=trainset, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "validationset_loader = torch.utils.data.DataLoader(dataset=validationset, \n",
    "                                                batch_size=batch_size, \n",
    "                                                shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = iter(trainset_loader)\n",
    "images,labels = dataiter.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images : torch.Size([32, 1, 28, 28]) type : torch.FloatTensor\n",
      "Labels : torch.Size([32]) type : torch.LongTensor\n"
     ]
    }
   ],
   "source": [
    "for images,labels in trainset_loader:\n",
    "    print('Images :',images.size(),'type :',images.type())\n",
    "    print('Labels :',labels.size(),'type :',labels.type())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "dark"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(images[0].numpy().squeeze(),cmap='gray');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 2.1042e+01, -2.3047e+01,  1.1772e+01,  2.4628e+01,  4.0001e+00,\n",
      "         -1.0738e+01, -2.0111e+01,  1.1074e+01, -5.7207e+00, -1.7006e+00],\n",
      "        [ 2.1733e+01, -2.9891e+00, -2.1630e+00,  1.4917e+01,  5.9713e+00,\n",
      "         -2.4532e+01, -4.4692e+00,  1.3889e+01, -1.1727e+00,  6.3640e+00],\n",
      "        [ 2.3038e+01, -1.7967e+01,  6.9164e+00,  1.9386e+01,  1.8640e+01,\n",
      "         -2.9081e+01,  4.6402e+00,  8.8282e+00, -1.0042e+01,  5.4575e+00],\n",
      "        [-1.0751e-01, -1.8057e+01,  1.6434e+01,  1.7638e+01,  1.0722e+01,\n",
      "         -9.7184e+00,  5.4665e+00,  1.7169e+01, -1.2430e+01,  5.2463e+00],\n",
      "        [ 9.4684e+00, -8.3443e+00,  6.3254e+00,  4.3738e+00,  2.1728e+01,\n",
      "         -1.0451e+01, -1.1627e+01,  1.4773e+01, -4.6736e+00, -1.5637e+00],\n",
      "        [ 1.4217e+01, -1.5185e+01, -8.2809e-01,  2.3803e+01,  1.3241e+01,\n",
      "         -2.3719e+01,  5.6984e+00,  2.0591e+01, -5.9919e+00,  5.2285e+00],\n",
      "        [ 1.6594e+01, -7.7942e+00,  5.2919e+00,  1.4358e+01,  6.8712e+00,\n",
      "         -2.6268e+01, -5.5392e+00,  1.0459e+01, -7.5001e+00,  3.7427e-01],\n",
      "        [ 4.8792e+00, -9.7797e+00, -1.0539e+00,  1.2570e+01,  2.3202e+01,\n",
      "         -2.2632e+01, -3.7746e-01,  1.9160e+01, -5.2346e+00,  6.7892e+00],\n",
      "        [ 2.5527e+01, -1.2744e+01,  1.0372e+00,  1.4638e+01,  2.5351e+00,\n",
      "         -1.5874e+01, -1.3156e+01,  3.0109e+01, -1.1221e+01,  1.0041e+00],\n",
      "        [ 2.2903e+01, -2.0570e+00, -2.5946e+00,  1.1879e+01,  1.5018e+01,\n",
      "         -1.2210e+01,  1.2716e-02,  2.3373e+01, -6.2084e+00, -2.3134e+00],\n",
      "        [ 1.9214e+01, -9.9210e+00, -3.0413e-01,  2.2566e+01,  1.5141e+01,\n",
      "         -2.0304e+01, -1.5378e+01,  1.2454e+01, -1.1860e+01,  4.2630e+00],\n",
      "        [ 2.1358e+01, -1.7263e+01,  7.8065e+00,  2.1634e+01,  7.5289e+00,\n",
      "         -1.6476e+01,  5.8105e+00, -2.2808e+00, -6.8646e+00, -1.1081e+00],\n",
      "        [ 2.1939e+01, -1.2152e+01,  4.6355e+00,  1.6343e+01,  2.3921e+01,\n",
      "         -1.4243e+01,  1.9204e+00,  1.3823e+01, -5.7929e+00,  4.6412e+00],\n",
      "        [-2.2841e+00, -2.1831e+01,  5.5956e+00,  2.0483e+01,  1.2799e+01,\n",
      "         -4.9996e+00, -1.5987e+00, -3.1994e+00, -5.9235e+00, -9.1194e-01],\n",
      "        [ 1.3292e+01, -2.4168e+01, -2.9662e-01,  2.9742e+01, -2.3491e+00,\n",
      "         -8.0493e+00, -1.5992e+01,  5.4441e+00, -1.0859e+01, -6.9953e+00],\n",
      "        [ 2.0843e+01,  2.0784e-01, -8.6454e-01,  2.6146e+01,  7.7971e+00,\n",
      "         -2.3589e+01, -1.1997e+01,  1.8979e+01, -3.2773e+00, -8.4287e-01],\n",
      "        [ 1.1826e+01, -2.5284e+01,  5.9859e+00,  2.4764e+01,  1.3940e+01,\n",
      "         -8.7173e+00, -1.2206e+01,  1.4347e+01, -6.6675e+00,  2.3425e+00],\n",
      "        [ 1.4043e+01, -2.7380e+01, -6.5265e+00,  2.3302e+01, -8.3355e-01,\n",
      "         -1.2377e+01, -2.8038e+01,  1.4392e+01, -4.6417e+00, -4.7043e+00],\n",
      "        [ 8.6981e+00, -5.4135e+00,  9.7638e-01,  1.5255e+01,  2.5867e+01,\n",
      "         -2.0316e+01, -5.2906e+00,  1.5550e+01, -9.5047e+00,  7.5057e+00],\n",
      "        [ 8.3102e+00, -1.4128e+01, -1.1929e-01,  1.1087e+01,  1.1755e+01,\n",
      "         -2.0848e+01, -5.0342e+00,  2.2696e+01, -7.7075e+00, -8.1374e-01],\n",
      "        [ 1.0972e+01, -1.6166e+01,  6.7492e+00,  1.6578e+01,  1.8462e+01,\n",
      "         -2.7711e+01,  2.0289e+00,  1.1083e+01, -2.0830e+00,  2.1778e+01],\n",
      "        [ 2.4800e+01, -1.8722e+01,  3.3775e+00,  2.4797e+01,  7.9295e+00,\n",
      "         -2.6203e+01, -7.0598e+00, -2.3867e-01, -1.2714e+01, -5.6380e+00],\n",
      "        [ 1.2353e+01, -1.7895e+01,  6.1526e-01,  1.6643e+01,  8.1729e+00,\n",
      "         -3.2746e+01, -5.8583e+00,  1.6960e+01, -1.1687e+01, -3.2948e+00],\n",
      "        [ 1.1542e+01, -1.4709e+01,  1.4134e+00,  3.4633e+01,  3.8835e+00,\n",
      "         -1.8521e+01, -6.6084e+00,  7.6394e+00, -1.0558e+00,  9.6853e-02],\n",
      "        [ 1.4400e+01, -2.6807e+01,  8.0014e+00,  2.8804e+01,  1.8216e+01,\n",
      "         -1.3476e+01, -1.2732e+01,  3.1881e+00, -1.8259e+01, -5.3498e+00],\n",
      "        [ 7.3897e+00, -1.0433e+01, -1.4317e+00,  2.9667e+00,  2.1709e+01,\n",
      "         -8.7137e+00,  5.7530e+00, -2.6081e-01, -7.2678e+00, -1.9735e+00],\n",
      "        [ 1.7960e+01, -1.8264e+01, -3.8173e-01,  2.4709e+01,  6.0258e+00,\n",
      "         -2.3476e+01, -6.5922e+00,  2.3783e+01, -7.8460e+00,  5.2801e+00],\n",
      "        [ 1.3451e+01,  6.0658e+00,  4.9385e+00,  1.0602e+01,  1.4965e+01,\n",
      "         -1.2144e+01, -8.7265e+00,  1.8766e-02,  4.1396e+00,  1.5699e+01],\n",
      "        [ 1.1137e+01, -1.0190e+01,  1.3127e+01,  1.4413e+01,  1.3968e+01,\n",
      "         -1.6509e+01, -3.7284e+00, -1.0354e+00, -2.8561e+00,  2.2311e+01],\n",
      "        [ 1.9430e+01, -1.7695e+01,  3.7929e+00,  2.0534e+01,  9.1272e+00,\n",
      "         -2.8887e+01, -1.3132e+01,  2.5258e+01, -1.0134e+01, -1.8167e+00],\n",
      "        [ 2.6280e+01, -3.8030e+00, -2.4510e+00,  1.5991e+01,  1.4898e+01,\n",
      "         -2.4003e+01,  3.3736e-01,  2.0911e+01, -8.9005e+00,  2.2201e+00],\n",
      "        [ 2.5469e+01, -7.3612e+00, -2.3760e+00,  2.1472e+01,  1.1039e+01,\n",
      "         -1.5593e+01, -8.5277e+00,  1.6924e+01, -4.7085e+00,  1.8265e+00]])\n"
     ]
    }
   ],
   "source": [
    "inputs = images.view(images.shape[0],-1)\n",
    "\n",
    "# createing the w1 and b1\n",
    "w1 = torch.randn(28*28,256) \n",
    "b1 = torch.randn(256)\n",
    "\n",
    "w2 = torch.randn(256,10)\n",
    "b2 = torch.randn(10)\n",
    "\n",
    "h = sigmoid(torch.mm(inputs,w1) + b1)\n",
    "\n",
    "out = torch.mm(h,w2) + b2\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    return torch.exp(x)/torch.sum(torch.exp(x),dim=1).view(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "probablities = softmax(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 10])\n"
     ]
    }
   ],
   "source": [
    "print(probablities.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000])\n"
     ]
    }
   ],
   "source": [
    "print(probablities.sum(dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super.__init__()\n",
    "        \n",
    "        self.hidden = nn.Linear(28*28,256)\n",
    "        self.output = nn.Linear(256,10)\n",
    "        \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.hidden(x)\n",
    "        x = self.sigmoid(x)\n",
    "        x = self.output(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "class NetFunctional(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hidden1 = nn.Linear(28*28,128)\n",
    "        self.hidden2 = nn.Linear(128,32)\n",
    "        self.output = nn.Linear(32,10)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = F.relu(self.hidden1(x))\n",
    "        x = F.relu(self.hidden2(x))\n",
    "        x = F.softmax(self.output(x))\n",
    "        return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(nn.Linear(28*28,128),\n",
    "                     nn.ReLU(),\n",
    "                     nn.Linear(128,32),\n",
    "                     nn.ReLU(),\n",
    "                     nn.LogSoftmax(dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "images,labels = next(iter(trainset_loader))\n",
    "images = images.view(images.shape[0],-1)\n",
    "# logit = NetFunctional()\n",
    "logit = model(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss = criterion(logit(images),labels)\n",
    "loss = criterion(logit,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.4828, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model[0].weight.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "print(model[0].weight.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
